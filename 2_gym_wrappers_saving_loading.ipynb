{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines3 Tutorial - Gym wrappers, saving and loading models\n",
    "- Save / load models.\n",
    "- Gym wrappers for monitoring, normalization, limit number of steps, feature augmentation.\n",
    "- About the saving format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: install swig\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre saved (array([-0.01560123], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create dir to save models.\n",
    "save_dir = '/tmp/gym/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Train and save a PPO model.\n",
    "model = PPO('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
    "model.save(save_dir + \"PPO_tutorial\")\n",
    "\n",
    "# Sample an observation from the environment and display it.\n",
    "obs = model.env.observation_space.sample()\n",
    "\n",
    "print(\"pre saved\", model.predict(obs, deterministic=True))\n",
    "del model # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded (array([-0.01560123], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "# Now we load the saved model and compare its prediction for the same observation.\n",
    "loaded_model = PPO.load(save_dir + 'PPO_tutorial')\n",
    "print(\"loaded\", loaded_model.predict(obs, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: gamma = 0.9 n_steps =  20\n"
     ]
    }
   ],
   "source": [
    "# Models are saved with training hyperparameters and current weights =>\n",
    "# You can load a custom model WITHOUT redefining the params. and continue learning.\n",
    "\n",
    "# Train and save an A2C model.\n",
    "model = A2C('MlpPolicy', 'Pendulum-v0', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
    "model.save(save_dir + \"A2C_tutorial\")\n",
    "\n",
    "del model # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the model, and when loading set verbose to 1\n",
    "loaded_model = A2C.load(save_dir + 'A2C_tutorial', verbose=1)\n",
    "# Show the saved hyperparameters (gamma and n_steps for envs. updates).\n",
    "print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps = \", loaded_model.n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.0252   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -39      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.2e+03  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.00121  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -36.2    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 675      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1289     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0.0112   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -21.7    |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 467      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1273     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.187    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 5.77     |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 290      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x13f8f4220>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's continue learning.\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# The env. was not serialized => we must assign a new instance to it.\n",
    "# 'DummyVecEnv' creates a simple vectorized wrapper for multiple environments for *sequential* execution.\n",
    "loaded_model.set_env(DummyVecEnv([lambda: gym.make('Pendulum-v0')]))\n",
    "# and continue training...\n",
    "loaded_model.learn(8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gym and environment wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a gym wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
    "\n",
    "A wrapper is *around* an environment => we can access it with `self.env` (interact with it without modifying original env).\n",
    "[List of predefined [gym wrappers](https://github.com/openai/gym/tree/master/gym/wrappers) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) to be wrapped.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # Call the parent constructor, to access self.env later.\n",
    "        super(CustomWrapper, self).__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment.\n",
    "        \"\"\"\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent.\n",
    "        \"\"\"\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: limit the episode length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimitWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    :param max_steps: (int) Max number of steps per episode\n",
    "    \"\"\"\n",
    "    def __init__(self, env, max_steps=100):\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super(TimeLimitWrapper, self).__init__(env)\n",
    "        self.max_steps = max_steps\n",
    "        # Counter of steps per episode\n",
    "        self.current_step = 0\n",
    "      \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment \n",
    "        \"\"\"\n",
    "        # Reset the counter\n",
    "        self.current_step = 0\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        # Overwrite the done signal if needed\n",
    "        if self.current_step >= self.max_steps:\n",
    "          done = True\n",
    "          # Update the info dict to signal that the limit was exceeded\n",
    "          info['time_limit_reached'] = True\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 {'time_limit_reached': True}\n"
     ]
    }
   ],
   "source": [
    "# Test the wrapper.\n",
    "\n",
    "from gym.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "# Here we create the environment directly because gym.make() already wraps the environement in a TimeLimit wrapper otherwise.\n",
    "env = PendulumEnv()\n",
    "env = TimeLimitWrapper(env, max_steps=100) # Wrap the environment.\n",
    "# In practice, `gym` already has that wrapper (`gym.wrappers.TimeLimit`).\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "n_steps = 0\n",
    "\n",
    "while not done:\n",
    "    random_action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(random_action)\n",
    "    n_steps += 1\n",
    "\n",
    "print(n_steps, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: normalize actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing observations and actions before input prevents [hard to debug issues](https://github.com/hill-a/stable-baselines/issues/473).\n",
    "\n",
    "Example: normalize the action space of *Pendulum-v0* so it lies in [-1, 1] instead of [-2, 2].\n",
    "\n",
    "Note: here we are dealing with continuous actions, hence the `gym.Box` space\n",
    "\n",
    "Approach:\n",
    "- Redefine env's action space to [-1, 1] for predictions from agent.\n",
    "- Use original rante of [-2, 2] for actual execution on env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env):\n",
    "    # Retrieve the action space\n",
    "    action_space = env.action_space\n",
    "    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
    "    # Retrieve the max/min values\n",
    "    self.low, self.high = action_space.low, action_space.high\n",
    "\n",
    "    # We modify the action space, so all actions will lie in [-1, 1]\n",
    "    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n",
    "\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(NormalizeActionWrapper, self).__init__(env)\n",
    "  \n",
    "  def rescale_action(self, scaled_action):\n",
    "      \"\"\"\n",
    "      Rescale the action from [-1, 1] to [low, high]\n",
    "      (no need for symmetric action space)\n",
    "      :param scaled_action: (np.ndarray)\n",
    "      :return: (np.ndarray)\n",
    "      \"\"\"\n",
    "      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    # Rescale action from [-1, 1] to original [low, high] interval.\n",
    "    rescaled_action = self.rescale_action(action)\n",
    "    obs, reward, done, info = self.env.step(rescaled_action)\n",
    "    return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-2.], [2.], (1,), float32)\n",
      "[-0.5280667]\n",
      "[-1.3647538]\n",
      "[0.2881489]\n",
      "[0.5634936]\n",
      "[-1.0391625]\n",
      "[1.3864685]\n",
      "[0.25647342]\n",
      "[-0.176545]\n",
      "[0.2872327]\n",
      "[-1.6624472]\n"
     ]
    }
   ],
   "source": [
    "# Test before rescaling actions\n",
    "original_env = gym.make('Pendulum-v0')\n",
    "\n",
    "print(original_env.action_space)\n",
    "for _ in range(10):\n",
    "    print(original_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1.], [1.], (1,), float32)\n",
      "[-0.4522421]\n",
      "[0.8005815]\n",
      "[-0.9332104]\n",
      "[-0.6878055]\n",
      "[-0.14478484]\n",
      "[-0.5603437]\n",
      "[0.26563168]\n",
      "[0.5935785]\n",
      "[-0.64743596]\n",
      "[0.19701849]\n"
     ]
    }
   ],
   "source": [
    "# Test the NormalizeActionWrapper\n",
    "env = NormalizeActionWrapper(gym.make('Pendulum-v0'))\n",
    "print(env.action_space)\n",
    "for _ in range(10):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor wrapper: training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an RL algorithm.\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = Monitor(gym.make('Pendulum-v0'))\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.49e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 806       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | -0.119    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -14.9     |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 218       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.29e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 821       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | 0.246     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -12.5     |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 131       |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training on the wrapped environment.\n",
    "model = A2C('MlpPolicy', env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.38e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 815       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | -0.0327   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -29.6     |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 793       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.31e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 823       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.41     |\n",
      "|    explained_variance | 0.476     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -7.01     |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 77.4      |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# And with the action wrapper.\n",
    "normalized_env = Monitor(gym.make('Pendulum-v0'))\n",
    "normalized_env = NormalizeActionWrapper(normalized_env)\n",
    "normalized_env = DummyVecEnv([lambda: normalized_env])\n",
    "\n",
    "model_2 = A2C('MlpPolicy', normalized_env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional wrappers: VecEnvWrappers\n",
    "* VecNormalize:\n",
    "    * It computes a running mean and standard deviation to normalize observation and returns.\n",
    "    * The running mean and std must be saved along with the model for it to work well when reloaded (rlzoo automates this).\n",
    "* VecFrameStack:\n",
    "    * It stacks several consecutive observations (e.g. successive frames on an Atari game)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.61541194 -0.94191027  0.9852751 ]] [-10.]\n",
      "[[-1.1067044 -1.2902389  1.392708 ]] [-2.0191271]\n",
      "[[-1.2992098 -1.4806343  1.4788196]] [-1.279137]\n",
      "[[-1.2608002 -1.5589188  1.2447139]] [-0.9771113]\n",
      "[[-1.0417725 -1.6381032  1.4008089]] [-0.80021083]\n",
      "[[-0.5759598  -1.62112     0.76637876]] [-0.66116494]\n",
      "[[ 0.18135568 -1.5977792   0.4901626 ]] [-0.5197159]\n",
      "[[ 1.1808501 -1.5855863  0.445956 ]] [-0.42672816]\n",
      "[[ 1.9036471  -1.5651357   0.22260936]] [-0.3601366]\n",
      "[[ 2.0568414  -1.49099    -0.88663495]] [-0.30871642]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make('Pendulum-v0')])\n",
    "normalized_vec_env = VecNormalize(env)\n",
    "\n",
    "obs = normalized_vec_env.reset()\n",
    "for _ in range(10):\n",
    "    action = [normalized_vec_env.action_space.sample()]\n",
    "    obs, reward, _, _ = normalized_vec_env.step(action)\n",
    "    print(obs, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: code your own monitor wrapper\n",
    "Create a wrapper to monitor the training process, storing both the episode reward (sum of reward for one episode) and episode length (number of steps of the last episode).\n",
    "You will return those values using the info dict after each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMonitorWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # Initialize attributes.\n",
    "        self.episode_reward = 0\n",
    "        self.episode_length = 0\n",
    "\n",
    "        # Call the parent constructor, so we can access self.env later.\n",
    "        super(MyMonitorWrapper, self).__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the attributes.\n",
    "        self.episode_reward = 0\n",
    "        self.episode_length = 0\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        # Run environment's step.\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.episode_reward += reward\n",
    "        self.episode_length += 1\n",
    "\n",
    "        # Check if the episode is finished, to update 'info'.\n",
    "        if done:\n",
    "            info['episode_reward'] = self.episode_reward\n",
    "            info['episode_length'] = self.episode_length\n",
    "\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TimeLimit.truncated': True, 'episode_reward': array([-966.1632], dtype=float32), 'episode_length': 200}\n",
      "{'TimeLimit.truncated': True, 'episode_reward': array([-1034.454], dtype=float32), 'episode_length': 200}\n",
      "{'TimeLimit.truncated': True, 'episode_reward': array([-1725.5581], dtype=float32), 'episode_length': 200}\n",
      "{'TimeLimit.truncated': True, 'episode_reward': array([-888.18964], dtype=float32), 'episode_length': 200}\n",
      "{'TimeLimit.truncated': True, 'episode_reward': array([-863.9017], dtype=float32), 'episode_length': 200}\n"
     ]
    }
   ],
   "source": [
    "# Test your wrapper.\n",
    "\n",
    "# Dependencies: install box2d box2d-kengz\n",
    "\n",
    "#env = gym.make('LunarLander-v2') # BUG\n",
    "env = gym.make('Pendulum-v0') # This one works.\n",
    "\n",
    "# Wrap the environment.\n",
    "monitored_env = MyMonitorWrapper(env)\n",
    "\n",
    "# Reset the environment.\n",
    "obs = monitored_env.reset()\n",
    "\n",
    "# Take random actions in the environment and check that\n",
    "# it returns the correct values after the end of each episode.\n",
    "for _ in range(1000):\n",
    "    action = [monitored_env.action_space.sample()]\n",
    "    obs, reward, done, info = monitored_env.step(action)\n",
    "    if done:\n",
    "        print(info)\n",
    "        monitored_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper bonus: changing observation space: a wrapper for episode of fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See code here:\n",
    "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/2_gym_wrappers_saving_loading.ipynb#scrollTo=bBlS9YxYSpJn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format for saving and loading models is a zip-archived JSON dump and NumPy zip archive of the arrays:\n",
    "```\n",
    "saved_model.zip/\n",
    "├── data              JSON file of class-parameters (dictionary)\n",
    "├── parameter_list    JSON file of model parameters and their ordering (list)\n",
    "├── parameters        Bytes from numpy.savez (a zip file of the numpy arrays). ...\n",
    "    ├── ...           Being a zip-archive itself, this object can also be opened ...\n",
    "        ├── ...       as a zip-archive and browsed.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create save dir\n",
    "save_dir = \"/tmp/gym/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = PPO('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
    "model.save(save_dir + \"/PPO_tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/gym/PPO_tutorial.zip\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/gym/PPO_tutorial*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "pytorch_variables.pth\n",
      "policy.pth\n",
      "policy.optimizer.pth\n",
      "_stable_baselines3_version\n",
      "system_info.txt\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "archive = zipfile.ZipFile(\"/tmp/gym/PPO_tutorial.zip\", 'r')\n",
    "for f in archive.filelist:\n",
    "  print(f.filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a25fab064cd93c11456bb88c41d634ca058856b88a784e3a94d171d100adf666"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
