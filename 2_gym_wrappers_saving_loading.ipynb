{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines3 Tutorial - Gym wrappers, saving and loading models\n",
    "- Save / load models.\n",
    "- Gym wrappers for monitoring, normalization, limit number of steps, feature augmentation.\n",
    "- Export models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: install swig\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre saved (array([0.21338284], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create dir to save models.\n",
    "save_dir = '/tmp/gym/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Train and save a PPO model.\n",
    "model = PPO('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n",
    "model.save(save_dir + \"PPO_tutorial\")\n",
    "\n",
    "# Sample an observation from the environment and display it.\n",
    "obs = model.env.observation_space.sample()\n",
    "\n",
    "print(\"pre saved\", model.predict(obs, deterministic=True))\n",
    "del model # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded (array([0.21338284], dtype=float32), None)\n"
     ]
    }
   ],
   "source": [
    "# Now we load the saved model and compare its prediction for the same observation.\n",
    "loaded_model = PPO.load(save_dir + 'PPO_tutorial')\n",
    "print(\"loaded\", loaded_model.predict(obs, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: gamma = 0.9 n_steps =  20\n"
     ]
    }
   ],
   "source": [
    "# Models are saved with training hyperparameters and current weights =>\n",
    "# You can load a custom model WITHOUT redefining the params. and continue learning.\n",
    "\n",
    "# Train and save a A2C model.\n",
    "model = A2C('MlpPolicy', 'Pendulum-v0', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n",
    "model.save(save_dir + \"A2C_tutorial\")\n",
    "\n",
    "del model # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the model, and when loading set verbose to 1\n",
    "loaded_model = A2C.load(save_dir + 'A2C_tutorial', verbose=1)\n",
    "# Show the saved hyperparameters (gamma and n_steps for envs. updates).\n",
    "print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps = \", loaded_model.n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 885       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | -4.89e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -52.9     |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 1.28e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 884      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.0251   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -23.7    |\n",
      "|    std                | 0.988    |\n",
      "|    value_loss         | 466      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 881      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.174    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -8.88    |\n",
      "|    std                | 0.987    |\n",
      "|    value_loss         | 463      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 869      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.291    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 17.1     |\n",
      "|    std                | 0.987    |\n",
      "|    value_loss         | 169      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x13dad1580>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's continue learning.\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# The env. was not serialized => we must assign a new instance to it.\n",
    "loaded_model.set_env(DummyVecEnv([lambda: gym.make('Pendulum-v0')]))\n",
    "# and continue training...\n",
    "loaded_model.learn(8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gym and VecEnv wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a gym wrapper\n",
    "\n",
    "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
    "\n",
    "A wrapper is *around* an environment => we can access it with `self.env` (interact with it without modifying original env).\n",
    "[List of predefined [gym wrappers](https://github.com/openai/gym/tree/master/gym/wrappers) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) to be wrapped.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # Call the parent constructor, to access self.env later.\n",
    "        super(CustomWrapper, self).__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment.\n",
    "        \"\"\"\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent.\n",
    "        \"\"\"\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First example: limit the episode length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimitWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    :param max_steps: (int) Max number of steps per episode\n",
    "    \"\"\"\n",
    "    def __init__(self, env, max_steps=100):\n",
    "        # Call the parent constructor, so we can access self.env later\n",
    "        super(TimeLimitWrapper, self).__init__(env)\n",
    "        self.max_steps = max_steps\n",
    "        # Counter of steps per episode\n",
    "        self.current_step = 0\n",
    "      \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment \n",
    "        \"\"\"\n",
    "        # Reset the counter\n",
    "        self.current_step = 0\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        :param action: ([float] or int) Action taken by the agent\n",
    "        :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        # Overwrite the done signal if needed\n",
    "        if self.current_step >= self.max_steps:\n",
    "          done = True\n",
    "          # Update the info dict to signal that the limit was exceeded\n",
    "          info['time_limit_reached'] = True\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 {'time_limit_reached': True}\n"
     ]
    }
   ],
   "source": [
    "# Test the wrapper.\n",
    "\n",
    "from gym.envs.classic_control.pendulum import PendulumEnv\n",
    "\n",
    "# Here we create the environment directly because gym.make() already wraps the environement in a TimeLimit wrapper otherwise\n",
    "env = PendulumEnv()\n",
    "env = TimeLimitWrapper(env, max_steps=100) # Wrap the environment.\n",
    "# In practice, `gym` already has that wrapper (`gym.wrappers.TimeLimit`).\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "n_steps = 0\n",
    "\n",
    "while not done:\n",
    "    random_action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(random_action)\n",
    "    n_steps += 1\n",
    "\n",
    "print(n_steps, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second example: normalize actions\n",
    "\n",
    "Normalizing observations and actions before input prevents [hard to debug issues](https://github.com/hill-a/stable-baselines/issues/473).\n",
    "\n",
    "Example: normalize the action space of *Pendulum-v0* so it lies in [-1, 1] instead of [-2, 2].\n",
    "\n",
    "Note: here we are dealing with continuous actions, hence the `gym.Box` space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "  \"\"\"\n",
    "  :param env: (gym.Env) Gym environment that will be wrapped\n",
    "  \"\"\"\n",
    "  def __init__(self, env):\n",
    "    # Retrieve the action space\n",
    "    action_space = env.action_space\n",
    "    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n",
    "    # Retrieve the max/min values\n",
    "    self.low, self.high = action_space.low, action_space.high\n",
    "\n",
    "    # We modify the action space, so all actions will lie in [-1, 1]\n",
    "    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n",
    "\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "    super(NormalizeActionWrapper, self).__init__(env)\n",
    "  \n",
    "  def rescale_action(self, scaled_action):\n",
    "      \"\"\"\n",
    "      Rescale the action from [-1, 1] to [low, high]\n",
    "      (no need for symmetric action space)\n",
    "      :param scaled_action: (np.ndarray)\n",
    "      :return: (np.ndarray)\n",
    "      \"\"\"\n",
    "      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment \n",
    "    \"\"\"\n",
    "    return self.env.reset()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    :param action: ([float] or int) Action taken by the agent\n",
    "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
    "    \"\"\"\n",
    "    # Rescale action from [-1, 1] to original [low, high] interval.\n",
    "    rescaled_action = self.rescale_action(action)\n",
    "    obs, reward, done, info = self.env.step(rescaled_action)\n",
    "    return obs, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-2.], [2.], (1,), float32)\n",
      "[1.9188143]\n",
      "[-1.2870473]\n",
      "[-0.5659662]\n",
      "[-1.4103976]\n",
      "[-1.7944611]\n",
      "[-1.0979551]\n",
      "[-1.9090642]\n",
      "[0.16198376]\n",
      "[0.44676518]\n",
      "[-1.0463634]\n"
     ]
    }
   ],
   "source": [
    "# Test before rescaling actions\n",
    "original_env = gym.make('Pendulum-v0')\n",
    "\n",
    "print(original_env.action_space)\n",
    "for _ in range(10):\n",
    "    print(original_env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1.], [1.], (1,), float32)\n",
      "[-0.21427454]\n",
      "[-0.30344495]\n",
      "[0.4193376]\n",
      "[-0.06500097]\n",
      "[-0.3769431]\n",
      "[-0.00690698]\n",
      "[0.961293]\n",
      "[0.3419016]\n",
      "[0.17041354]\n",
      "[-0.7051441]\n"
     ]
    }
   ],
   "source": [
    "env = NormalizeActionWrapper(gym.make('Pendulum-v0'))\n",
    "print(env.action_space)\n",
    "for _ in range(10):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor wrapper: training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an RL algorithm.\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = Monitor(gym.make('Pendulum-v0'))\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the wrapped environment.\n",
    "model = A2C('MlpPolicy', env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.22e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 520       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 0.0592    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -50.3     |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.66e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 200       |\n",
      "|    ep_rew_mean        | -1.28e+03 |\n",
      "| time/                 |           |\n",
      "|    fps                | 531       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 0.175     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -46.3     |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 712       |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# And with the action wrapper.\n",
    "normalized_env = Monitor(gym.make('Pendulum-v0'))\n",
    "normalized_env = NormalizeActionWrapper(normalized_env)\n",
    "normalized_env = DummyVecEnv([lambda: normalized_env])\n",
    "\n",
    "model_2 = A2C('MlpPolicy', normalized_env, verbose=1).learn(int(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a25fab064cd93c11456bb88c41d634ca058856b88a784e3a94d171d100adf666"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
