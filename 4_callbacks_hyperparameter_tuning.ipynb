{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines3 Tutorial - Callbacks and hyperparameter tuning\n",
    "(Taken from <https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#>)\n",
    "- Comparing default and beest hyperparameters in RL.\n",
    "- Using callbacks for monitoring, auto-saving, model manipulation, progress bars...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: swig, tqdm\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter tuning\n",
    "We'll compare here the performance of \"Soft Actor Critic\" on the Pendulum environment with default and \"tuned\" hyperparameters.\n",
    "\n",
    "Resources:\n",
    "- rl zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "- Optuna: https://github.com/optuna/optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = Monitor(gym.make('Pendulum-v0')) # AH: Wrapped with Monitor to prevent erroneous metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v0'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.36e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 125       |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 20.3      |\n",
      "|    critic_loss     | 0.968     |\n",
      "|    ent_coef        | 0.812     |\n",
      "|    ent_coef_loss   | -0.337    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.55e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 121       |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 43.2      |\n",
      "|    critic_loss     | 0.943     |\n",
      "|    ent_coef        | 0.644     |\n",
      "|    ent_coef_loss   | -0.662    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.52e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 120       |\n",
      "|    time_elapsed    | 19        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 73        |\n",
      "|    critic_loss     | 0.875     |\n",
      "|    ent_coef        | 0.516     |\n",
      "|    ent_coef_loss   | -0.886    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.39e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 120       |\n",
      "|    time_elapsed    | 26        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 84.9      |\n",
      "|    critic_loss     | 7.05      |\n",
      "|    ent_coef        | 0.422     |\n",
      "|    ent_coef_loss   | -0.875    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.36e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 119       |\n",
      "|    time_elapsed    | 33        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 98.2      |\n",
      "|    critic_loss     | 11.2      |\n",
      "|    ent_coef        | 0.359     |\n",
      "|    ent_coef_loss   | -0.813    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.3e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 116      |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.315    |\n",
      "|    ent_coef_loss   | -0.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.27e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 120       |\n",
      "|    time_elapsed    | 46        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 135       |\n",
      "|    critic_loss     | 8.01      |\n",
      "|    ent_coef        | 0.269     |\n",
      "|    ent_coef_loss   | -0.561    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 120       |\n",
      "|    time_elapsed    | 53        |\n",
      "|    total_timesteps | 6400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 136       |\n",
      "|    critic_loss     | 6.61      |\n",
      "|    ent_coef        | 0.228     |\n",
      "|    ent_coef_loss   | -0.352    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 6299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.12e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 119       |\n",
      "|    time_elapsed    | 60        |\n",
      "|    total_timesteps | 7200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 158       |\n",
      "|    critic_loss     | 6.29      |\n",
      "|    ent_coef        | 0.2       |\n",
      "|    ent_coef_loss   | -0.19     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 119       |\n",
      "|    time_elapsed    | 66        |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 172       |\n",
      "|    critic_loss     | 6.62      |\n",
      "|    ent_coef        | 0.184     |\n",
      "|    ent_coef_loss   | 0.133     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7899      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "default_model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1, seed=0, batch_size=64, policy_kwargs=dict(net_arch=[64, 64])).learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-182.25 +/- 99.20\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(default_model, eval_env, n_eval_episodes=500)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v0'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.56e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 51        |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 24.8      |\n",
      "|    critic_loss     | 0.259     |\n",
      "|    ent_coef        | 0.814     |\n",
      "|    ent_coef_loss   | -0.339    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.61e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 47        |\n",
      "|    time_elapsed    | 33        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 54.8      |\n",
      "|    critic_loss     | 0.13      |\n",
      "|    ent_coef        | 0.643     |\n",
      "|    ent_coef_loss   | -0.661    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.43e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 51        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 70.6      |\n",
      "|    critic_loss     | 0.397     |\n",
      "|    ent_coef        | 0.524     |\n",
      "|    ent_coef_loss   | -0.626    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.3e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 44       |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 82.5     |\n",
      "|    critic_loss     | 0.602    |\n",
      "|    ent_coef        | 0.45     |\n",
      "|    ent_coef_loss   | -0.533   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.08e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 43        |\n",
      "|    time_elapsed    | 90        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 79        |\n",
      "|    critic_loss     | 0.732     |\n",
      "|    ent_coef        | 0.4       |\n",
      "|    ent_coef_loss   | -0.359    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -942     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 85.5     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.345    |\n",
      "|    ent_coef_loss   | -0.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -833     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 79.4     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.288    |\n",
      "|    ent_coef_loss   | -0.687   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -759     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 86.5     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.238    |\n",
      "|    ent_coef_loss   | -0.549   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -692     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 78.1     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.201    |\n",
      "|    ent_coef_loss   | -0.391   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -638     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 182      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 77.2     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.174    |\n",
      "|    ent_coef_loss   | -0.373   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "tuned_model = SAC('MlpPolicy', 'Pendulum-v0', batch_size=256, verbose=1, policy_kwargs=dict(net_arch=[256, 256]), seed=0).learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-148.26 +/- 91.90\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(tuned_model, eval_env, n_eval_episodes=500)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Callbacks\n",
    "Callback = function that will be called at a given stage of the training.\n",
    "They are passed as an argument of `model.learn()`.\n",
    "\n",
    "Types:\n",
    "* Custom callback: called at 5 specific moments of the training process.\n",
    "* Event callback: called when a certain user-defined situation is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Custom callback\n",
    "The class derives from `BaseCallback`.\n",
    "\n",
    "Events:\n",
    "* `_on_training_start`  Called before the first rollout starts.\n",
    "* `_on_rollout_start`   \n",
    "    * Rollout = collection of environment interactions using current policy.\n",
    "    * Triggered before collecting new samples.\n",
    "    * For off-policy algorithms, rollout = steps taken in the eenv between two updates.\n",
    "* `_on_step`\n",
    "    * Called by the model after each call to `env.step()`.\n",
    "    * For child callback (of an `EventCallback`), this is called when the event is triggered.\n",
    "    * :return: (bool) If False, training is aborted early.\n",
    "* `_on_rollout_end`     Triggered before updating the policy.\n",
    "* `_on_training_end`    Triggered before exiting the `learn()` method.\n",
    "\n",
    "Variables accessible in the callback:\n",
    "* `self.model`          The RL model (`type: BaseAlgorithm`).\n",
    "* `self.training_env`   The environment used for training (`type Union[gym.Env, VecEnv, None]`).\n",
    "* `self.n_calls`        Number of times the callback was called (`type int`).\n",
    "* `self.num_timesteps`  Total number of steps taken (number of envs x step calls) (`type int`).\n",
    "* `self.locals`         Local variables (`type: Dict[str, Any]`).\n",
    "* `self.globals`        Global variables (`type: Dict[str, Any]`).\n",
    "* `self.logger`         The logger object, to report on terminal (`type: stable_baselines3.common.logger`).\n",
    "* `self.parent`         The parent object (`type: Optional[BaseCallback]`).\n",
    "\n",
    "### 2.2 Event callback\n",
    "The class `EventCallback` derives from `BaseCallback`.\n",
    "When an event is triggered (e.g. `EvalCallback` when there's a new best model) =>\n",
    "a child callback is called (e.g. `StopTrainingOnRewardThreshold` if mean reward > a threshold).\n",
    "\n",
    "Callback collection:\n",
    "* Save the model periodically (`CheckpointCallback`)\n",
    "* Evaluate the model periodically and save the best one (`EvalCallback`)\n",
    "* Chain callbacks (`CallbackList`)\n",
    "* Trigger callback on events (`Event Callback`, `EveryNTimesteps`)\n",
    "* Stop training early based on a reward threshold (`StopTrainingOnRewardThreshold`)\n",
    "\n",
    "(Note: when using multiple envs. the frequence must be calculated like this: `save_freq = max(save_freq // n_envs, 1)`\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheckpointCallback\n",
    "Save the model periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x13de4ed60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=1000, save_path='./logs/', name_prefix='rl_model')\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0')\n",
    "model.learn(2000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EvalCallback\n",
    "Evaluate the model periodically and save the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-1296.92 +/- 46.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-1654.46 +/- 110.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=-1480.33 +/- 115.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=-1111.86 +/- 71.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=-660.48 +/- 110.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-1141.39 +/- 512.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=-912.03 +/- 590.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-297.22 +/- 99.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=-171.75 +/- 60.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-167.10 +/- 57.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x13dabcfd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('Pendulum-v0')\n",
    "# Use deterministic actions for evaluation\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, best_model_save_path='./logs/',\n",
    "    log_path='./logs/', eval_freq=500,\n",
    "    deterministic=True, render=False)\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0')\n",
    "model.learn(5000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CallbackList\n",
    "A list of chained callbacks will be called sequentially. Alternatively pass a list of callbacks to `learn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-1674.55 +/- 186.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-1790.63 +/- 140.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=-1355.32 +/- 105.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1041.60 +/- 99.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=-717.63 +/- 126.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-811.61 +/- 357.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=-153.40 +/- 52.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-223.64 +/- 90.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-120.91 +/- 72.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-125.08 +/- 77.56\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x13de95790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/')\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('Pendulum-v0')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/best_model',\n",
    "                             log_path='./logs/results', eval_freq=500)\n",
    "# Create the callback list\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0')\n",
    "# Equivalent to:\n",
    "# model.learn(5000, callback=[checkpoint_callback, eval_callback])\n",
    "model.learn(5000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StopTrainingOnRewardThreshold\n",
    "Stop training early based on a reward threshold.\n",
    "\n",
    "It must be used with the EvalCallback and use the event triggered by a new best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('Pendulum-v0')\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-200, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "# Almost infinite number of timesteps, but the training will stop\n",
    "# early as soon as the reward threshold is reached\n",
    "model.learn(int(1e10), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EveryNTimesteps\n",
    "Trigger its child callback every n_steps timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EveryNTimesteps\n",
    "\n",
    "# this is equivalent to defining CheckpointCallback(save_freq=500)\n",
    "# checkpoint_callback will be triggered every 500 steps\n",
    "checkpoint_on_event = CheckpointCallback(save_freq=1, save_path='./logs/')\n",
    "event_callback = EveryNTimesteps(n_steps=500, callback=checkpoint_on_event)\n",
    "\n",
    "model = PPO('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "\n",
    "model.learn(int(2e4), callback=event_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StopTrainingOnMaxEpisodes\n",
    "Stop training at a maximum number of episodes (ignoring model’s total_timesteps).\n",
    "\n",
    "Note: For multiple environments it assumes max_episodes * n_envs episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "# Stops training when the model reaches the maximum number of episodes\n",
    "callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=5, verbose=1)\n",
    "\n",
    "model = A2C('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "# Almost infinite number of timesteps, but the training will stop\n",
    "# early as soon as the max number of episodes is reached\n",
    "model.learn(int(1e10), callback=callback_max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Auto-saving best training model.\n",
    "* Approach: to observe mean training reward over time.\n",
    "* NOTE: the right approach would be to evaluate the model on a test environment in `EvalCallback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model based on the training reward.\n",
    "\n",
    "    :param check_freq: (int) The check is done every \"check_freq\" steps.\n",
    "    :param log_dir: (str) Path to folder where model will be saved.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq, log_dir, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed.\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward.\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes.\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(f\"Best mean reward: {self.best_mean_reward:.2} - Last mean reward per episode: {mean_reward:.2}\")\n",
    "                \n",
    "                # New best model, you could save the agent here.\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model.\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model at {x[-1]} timesteps to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 20\n",
      "Best mean reward: -inf - Last mean reward per episode: 2e+01\n",
      "Saving new best model at 20 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 40\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.6e+01\n",
      "Num timesteps: 60\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.6e+01\n",
      "Num timesteps: 80\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.7e+01\n",
      "Num timesteps: 100\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.7e+01\n",
      "Num timesteps: 120\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.8e+01\n",
      "Num timesteps: 140\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.7e+01\n",
      "Num timesteps: 160\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.8e+01\n",
      "Num timesteps: 180\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.9e+01\n",
      "Num timesteps: 200\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 1.9e+01\n",
      "Num timesteps: 220\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 2e+01\n",
      "Saving new best model at 203 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 240\n",
      "Best mean reward: 2e+01 - Last mean reward per episode: 2.1e+01\n",
      "Saving new best model at 233 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 260\n",
      "Best mean reward: 2.1e+01 - Last mean reward per episode: 2.1e+01\n",
      "Num timesteps: 280\n",
      "Best mean reward: 2.1e+01 - Last mean reward per episode: 2.3e+01\n",
      "Saving new best model at 272 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 300\n",
      "Best mean reward: 2.3e+01 - Last mean reward per episode: 2.3e+01\n",
      "Num timesteps: 320\n",
      "Best mean reward: 2.3e+01 - Last mean reward per episode: 2.4e+01\n",
      "Saving new best model at 308 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 340\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Saving new best model at 334 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 360\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.3e+01\n",
      "Num timesteps: 380\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.2e+01\n",
      "Num timesteps: 400\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.2e+01\n",
      "Num timesteps: 420\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.2e+01\n",
      "Num timesteps: 440\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.2e+01\n",
      "Num timesteps: 460\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.2e+01\n",
      "Num timesteps: 480\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.2e+01\n",
      "Num timesteps: 500\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 520\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 540\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Saving new best model at 534 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 560\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 580\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 600\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 620\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Saving new best model at 608 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 640\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 660\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Saving new best model at 659 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 680\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 700\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 720\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 740\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 760\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.4e+01\n",
      "Num timesteps: 780\n",
      "Best mean reward: 2.4e+01 - Last mean reward per episode: 2.5e+01\n",
      "Saving new best model at 772 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 800\n",
      "Best mean reward: 2.5e+01 - Last mean reward per episode: 2.5e+01\n",
      "Num timesteps: 820\n",
      "Best mean reward: 2.5e+01 - Last mean reward per episode: 2.5e+01\n",
      "Saving new best model at 815 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 840\n",
      "Best mean reward: 2.5e+01 - Last mean reward per episode: 2.5e+01\n",
      "Num timesteps: 860\n",
      "Best mean reward: 2.5e+01 - Last mean reward per episode: 2.6e+01\n",
      "Saving new best model at 845 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 880\n",
      "Best mean reward: 2.6e+01 - Last mean reward per episode: 2.6e+01\n",
      "Num timesteps: 900\n",
      "Best mean reward: 2.6e+01 - Last mean reward per episode: 2.6e+01\n",
      "Saving new best model at 884 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 920\n",
      "Best mean reward: 2.6e+01 - Last mean reward per episode: 2.6e+01\n",
      "Num timesteps: 940\n",
      "Best mean reward: 2.6e+01 - Last mean reward per episode: 2.6e+01\n",
      "Saving new best model at 921 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 960\n",
      "Best mean reward: 2.6e+01 - Last mean reward per episode: 2.7e+01\n",
      "Saving new best model at 958 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 980\n",
      "Best mean reward: 2.7e+01 - Last mean reward per episode: 2.7e+01\n",
      "Num timesteps: 1000\n",
      "Best mean reward: 2.7e+01 - Last mean reward per episode: 2.7e+01\n",
      "Num timesteps: 1020\n",
      "Best mean reward: 2.7e+01 - Last mean reward per episode: 2.8e+01\n",
      "Saving new best model at 1019 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1040\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1060\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.7e+01\n",
      "Num timesteps: 1080\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1100\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1120\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Saving new best model at 1120 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1140\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1160\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1180\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1200\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Saving new best model at 1186 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1220\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1240\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1260\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1280\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.8e+01\n",
      "Num timesteps: 1300\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 2.9e+01\n",
      "Saving new best model at 1292 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1320\n",
      "Best mean reward: 2.9e+01 - Last mean reward per episode: 2.9e+01\n",
      "Num timesteps: 1340\n",
      "Best mean reward: 2.9e+01 - Last mean reward per episode: 2.9e+01\n",
      "Num timesteps: 1360\n",
      "Best mean reward: 2.9e+01 - Last mean reward per episode: 2.9e+01\n",
      "Num timesteps: 1380\n",
      "Best mean reward: 2.9e+01 - Last mean reward per episode: 2.9e+01\n",
      "Num timesteps: 1400\n",
      "Best mean reward: 2.9e+01 - Last mean reward per episode: 2.9e+01\n",
      "Num timesteps: 1420\n",
      "Best mean reward: 2.9e+01 - Last mean reward per episode: 3e+01\n",
      "Saving new best model at 1404 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1440\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1460\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1480\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1500\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Saving new best model at 1489 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1520\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1540\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1560\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1580\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1600\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1620\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Saving new best model at 1615 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1640\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3e+01\n",
      "Num timesteps: 1660\n",
      "Best mean reward: 3e+01 - Last mean reward per episode: 3.1e+01\n",
      "Saving new best model at 1649 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1680\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1700\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1720\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Saving new best model at 1701 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1740\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Saving new best model at 1732 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1760\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1780\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1800\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1820\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1840\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1860\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Saving new best model at 1856 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1880\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1900\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1920\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Saving new best model at 1920 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 1940\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1960\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 1980\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 2020\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 2040\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.1e+01\n",
      "Num timesteps: 2060\n",
      "Best mean reward: 3.1e+01 - Last mean reward per episode: 3.2e+01\n",
      "Saving new best model at 2041 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2080\n",
      "Best mean reward: 3.2e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2100\n",
      "Best mean reward: 3.2e+01 - Last mean reward per episode: 3.2e+01\n",
      "Saving new best model at 2081 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2120\n",
      "Best mean reward: 3.2e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2140\n",
      "Best mean reward: 3.2e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2160\n",
      "Best mean reward: 3.2e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2180\n",
      "Best mean reward: 3.2e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2200\n",
      "Best mean reward: 3.2e+01 - Last mean reward per episode: 3.3e+01\n",
      "Saving new best model at 2185 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2220\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2240\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2260\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2280\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2300\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2320\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2340\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Saving new best model at 2329 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2360\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2380\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2400\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Saving new best model at 2396 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2420\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2440\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2460\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2480\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Saving new best model at 2466 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2500\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2520\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2540\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2560\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2580\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2600\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2620\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2640\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2660\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2680\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2700\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2720\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2740\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2760\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2780\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2800\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2820\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2840\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.2e+01\n",
      "Num timesteps: 2860\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Saving new best model at 2847 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2880\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2900\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2920\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Saving new best model at 2901 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2940\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Saving new best model at 2939 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 2960\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 2980\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3020\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.4e+01\n",
      "Saving new best model at 3006 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3040\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3060\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3080\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3100\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3120\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3140\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3160\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3180\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Saving new best model at 3178 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3200\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3220\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3240\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3260\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3280\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3300\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3320\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3340\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3360\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3380\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.3e+01\n",
      "Num timesteps: 3400\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3420\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3440\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3460\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Saving new best model at 3447 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3480\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3500\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Saving new best model at 3500 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3520\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3540\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Saving new best model at 3529 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3560\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3580\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Saving new best model at 3567 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3600\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3620\n",
      "Best mean reward: 3.4e+01 - Last mean reward per episode: 3.5e+01\n",
      "Saving new best model at 3615 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3640\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Num timesteps: 3660\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3680\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3700\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.4e+01\n",
      "Num timesteps: 3720\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Saving new best model at 3718 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3740\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Num timesteps: 3760\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Num timesteps: 3780\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Num timesteps: 3800\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Saving new best model at 3799 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3820\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Num timesteps: 3840\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Num timesteps: 3860\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Num timesteps: 3880\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Saving new best model at 3870 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3900\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Num timesteps: 3920\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.5e+01\n",
      "Saving new best model at 3905 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3940\n",
      "Best mean reward: 3.5e+01 - Last mean reward per episode: 3.6e+01\n",
      "Saving new best model at 3938 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 3960\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 3980\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4020\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4040\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 3.6e+01\n",
      "Saving new best model at 4037 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4060\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4080\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 3.6e+01\n",
      "Saving new best model at 4061 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4100\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 3.7e+01\n",
      "Saving new best model at 4099 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4120\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.7e+01\n",
      "Num timesteps: 4140\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4160\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4180\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4200\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4220\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.6e+01\n",
      "Num timesteps: 4240\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.7e+01\n",
      "Saving new best model at 4236 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4260\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.7e+01\n",
      "Num timesteps: 4280\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.7e+01\n",
      "Num timesteps: 4300\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.7e+01\n",
      "Num timesteps: 4320\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.7e+01\n",
      "Saving new best model at 4301 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4340\n",
      "Best mean reward: 3.7e+01 - Last mean reward per episode: 3.8e+01\n",
      "Saving new best model at 4335 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4360\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Num timesteps: 4380\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Saving new best model at 4380 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4400\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Num timesteps: 4420\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Num timesteps: 4440\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Num timesteps: 4460\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Saving new best model at 4443 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4480\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Num timesteps: 4500\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Num timesteps: 4520\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Saving new best model at 4506 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4540\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.8e+01\n",
      "Num timesteps: 4560\n",
      "Best mean reward: 3.8e+01 - Last mean reward per episode: 3.9e+01\n",
      "Saving new best model at 4552 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4580\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4600\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Saving new best model at 4584 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4620\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4640\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4660\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Saving new best model at 4647 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4680\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4700\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4720\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Saving new best model at 4703 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4740\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4760\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4780\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Saving new best model at 4763 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4800\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4820\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 3.9e+01\n",
      "Num timesteps: 4840\n",
      "Best mean reward: 3.9e+01 - Last mean reward per episode: 4e+01\n",
      "Saving new best model at 4829 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4860\n",
      "Best mean reward: 4e+01 - Last mean reward per episode: 4e+01\n",
      "Num timesteps: 4880\n",
      "Best mean reward: 4e+01 - Last mean reward per episode: 4e+01\n",
      "Num timesteps: 4900\n",
      "Best mean reward: 4e+01 - Last mean reward per episode: 4e+01\n",
      "Num timesteps: 4920\n",
      "Best mean reward: 4e+01 - Last mean reward per episode: 4e+01\n",
      "Num timesteps: 4940\n",
      "Best mean reward: 4e+01 - Last mean reward per episode: 4e+01\n",
      "Num timesteps: 4960\n",
      "Best mean reward: 4e+01 - Last mean reward per episode: 4.1e+01\n",
      "Saving new best model at 4943 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 4980\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Saving new best model at 4988 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 5020\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Num timesteps: 5040\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Num timesteps: 5060\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Num timesteps: 5080\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Num timesteps: 5100\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Num timesteps: 5120\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Num timesteps: 5140\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.1e+01\n",
      "Num timesteps: 5160\n",
      "Best mean reward: 4.1e+01 - Last mean reward per episode: 4.2e+01\n",
      "Saving new best model at 5142 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 5180\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5200\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5220\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5240\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5260\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Saving new best model at 5256 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 5280\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5300\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5320\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5340\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5360\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5380\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.2e+01\n",
      "Num timesteps: 5400\n",
      "Best mean reward: 4.2e+01 - Last mean reward per episode: 4.4e+01\n",
      "Saving new best model at 5393 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 5420\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5440\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5460\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5480\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5500\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5520\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Saving new best model at 5516 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 5540\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5560\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5580\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5600\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5620\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5640\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5660\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5680\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5700\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.4e+01\n",
      "Num timesteps: 5720\n",
      "Best mean reward: 4.4e+01 - Last mean reward per episode: 4.6e+01\n",
      "Saving new best model at 5712 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 5740\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.6e+01\n",
      "Num timesteps: 5760\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.6e+01\n",
      "Num timesteps: 5780\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.6e+01\n",
      "Saving new best model at 5776 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 5800\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.6e+01\n",
      "Num timesteps: 5820\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.6e+01\n",
      "Num timesteps: 5840\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.6e+01\n",
      "Num timesteps: 5860\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.6e+01\n",
      "Num timesteps: 5880\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.6e+01\n",
      "Num timesteps: 5900\n",
      "Best mean reward: 4.6e+01 - Last mean reward per episode: 4.7e+01\n",
      "Saving new best model at 5889 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 5920\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 5940\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 5960\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 5980\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6020\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6040\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6060\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6080\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6100\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6120\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6140\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6160\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6180\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6200\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6220\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6240\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6260\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6280\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6300\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6320\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6340\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6360\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6380\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 4.7e+01\n",
      "Num timesteps: 6400\n",
      "Best mean reward: 4.7e+01 - Last mean reward per episode: 5.2e+01\n",
      "Saving new best model at 6389 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 6420\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6440\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6460\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6480\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6500\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6520\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6540\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6560\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6580\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6600\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6620\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6640\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6660\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.2e+01\n",
      "Num timesteps: 6680\n",
      "Best mean reward: 5.2e+01 - Last mean reward per episode: 5.4e+01\n",
      "Saving new best model at 6669 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 6700\n",
      "Best mean reward: 5.4e+01 - Last mean reward per episode: 5.4e+01\n",
      "Num timesteps: 6720\n",
      "Best mean reward: 5.4e+01 - Last mean reward per episode: 5.4e+01\n",
      "Num timesteps: 6740\n",
      "Best mean reward: 5.4e+01 - Last mean reward per episode: 5.4e+01\n",
      "Num timesteps: 6760\n",
      "Best mean reward: 5.4e+01 - Last mean reward per episode: 5.4e+01\n",
      "Num timesteps: 6780\n",
      "Best mean reward: 5.4e+01 - Last mean reward per episode: 5.4e+01\n",
      "Num timesteps: 6800\n",
      "Best mean reward: 5.4e+01 - Last mean reward per episode: 5.5e+01\n",
      "Saving new best model at 6797 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 6820\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 6840\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 6860\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 6880\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 6900\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 6920\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 6940\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 6960\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 6980\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 7020\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 7040\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 7060\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 7080\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 7100\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 7120\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.5e+01\n",
      "Num timesteps: 7140\n",
      "Best mean reward: 5.5e+01 - Last mean reward per episode: 5.8e+01\n",
      "Saving new best model at 7131 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 7160\n",
      "Best mean reward: 5.8e+01 - Last mean reward per episode: 5.8e+01\n",
      "Num timesteps: 7180\n",
      "Best mean reward: 5.8e+01 - Last mean reward per episode: 5.8e+01\n",
      "Num timesteps: 7200\n",
      "Best mean reward: 5.8e+01 - Last mean reward per episode: 5.8e+01\n",
      "Num timesteps: 7220\n",
      "Best mean reward: 5.8e+01 - Last mean reward per episode: 5.8e+01\n",
      "Num timesteps: 7240\n",
      "Best mean reward: 5.8e+01 - Last mean reward per episode: 5.8e+01\n",
      "Num timesteps: 7260\n",
      "Best mean reward: 5.8e+01 - Last mean reward per episode: 5.8e+01\n",
      "Num timesteps: 7280\n",
      "Best mean reward: 5.8e+01 - Last mean reward per episode: 5.8e+01\n",
      "Num timesteps: 7300\n",
      "Best mean reward: 5.8e+01 - Last mean reward per episode: 5.9e+01\n",
      "Saving new best model at 7291 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 7320\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7340\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7360\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7380\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7400\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7420\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7440\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7460\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7480\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7500\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7520\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7540\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7560\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 5.9e+01\n",
      "Num timesteps: 7580\n",
      "Best mean reward: 5.9e+01 - Last mean reward per episode: 6.1e+01\n",
      "Saving new best model at 7575 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 7600\n",
      "Best mean reward: 6.1e+01 - Last mean reward per episode: 6.1e+01\n",
      "Num timesteps: 7620\n",
      "Best mean reward: 6.1e+01 - Last mean reward per episode: 6.1e+01\n",
      "Num timesteps: 7640\n",
      "Best mean reward: 6.1e+01 - Last mean reward per episode: 6.1e+01\n",
      "Num timesteps: 7660\n",
      "Best mean reward: 6.1e+01 - Last mean reward per episode: 6.1e+01\n",
      "Num timesteps: 7680\n",
      "Best mean reward: 6.1e+01 - Last mean reward per episode: 6.2e+01\n",
      "Saving new best model at 7674 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 7700\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7720\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7740\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7760\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7780\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7800\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7820\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7840\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7860\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7880\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7900\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7920\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7940\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7960\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 7980\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 8020\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 8040\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 8060\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.2e+01\n",
      "Num timesteps: 8080\n",
      "Best mean reward: 6.2e+01 - Last mean reward per episode: 6.6e+01\n",
      "Saving new best model at 8071 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 8100\n",
      "Best mean reward: 6.6e+01 - Last mean reward per episode: 6.6e+01\n",
      "Num timesteps: 8120\n",
      "Best mean reward: 6.6e+01 - Last mean reward per episode: 6.6e+01\n",
      "Num timesteps: 8140\n",
      "Best mean reward: 6.6e+01 - Last mean reward per episode: 6.6e+01\n",
      "Num timesteps: 8160\n",
      "Best mean reward: 6.6e+01 - Last mean reward per episode: 6.6e+01\n",
      "Num timesteps: 8180\n",
      "Best mean reward: 6.6e+01 - Last mean reward per episode: 6.6e+01\n",
      "Num timesteps: 8200\n",
      "Best mean reward: 6.6e+01 - Last mean reward per episode: 6.6e+01\n",
      "Num timesteps: 8220\n",
      "Best mean reward: 6.6e+01 - Last mean reward per episode: 6.7e+01\n",
      "Saving new best model at 8215 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 8240\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8260\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8280\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8300\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8320\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8340\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8360\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8380\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8400\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8420\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8440\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8460\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.7e+01\n",
      "Num timesteps: 8480\n",
      "Best mean reward: 6.7e+01 - Last mean reward per episode: 6.9e+01\n",
      "Saving new best model at 8476 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 8500\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8520\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8540\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8560\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8580\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8600\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8620\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8640\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8660\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8680\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8700\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8720\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 6.9e+01\n",
      "Num timesteps: 8740\n",
      "Best mean reward: 6.9e+01 - Last mean reward per episode: 7.1e+01\n",
      "Saving new best model at 8731 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 8760\n",
      "Best mean reward: 7.1e+01 - Last mean reward per episode: 7.1e+01\n",
      "Num timesteps: 8780\n",
      "Best mean reward: 7.1e+01 - Last mean reward per episode: 7.1e+01\n",
      "Num timesteps: 8800\n",
      "Best mean reward: 7.1e+01 - Last mean reward per episode: 7.1e+01\n",
      "Num timesteps: 8820\n",
      "Best mean reward: 7.1e+01 - Last mean reward per episode: 7.1e+01\n",
      "Num timesteps: 8840\n",
      "Best mean reward: 7.1e+01 - Last mean reward per episode: 7.1e+01\n",
      "Num timesteps: 8860\n",
      "Best mean reward: 7.1e+01 - Last mean reward per episode: 7.1e+01\n",
      "Num timesteps: 8880\n",
      "Best mean reward: 7.1e+01 - Last mean reward per episode: 7.2e+01\n",
      "Saving new best model at 8868 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 8900\n",
      "Best mean reward: 7.2e+01 - Last mean reward per episode: 7.2e+01\n",
      "Num timesteps: 8920\n",
      "Best mean reward: 7.2e+01 - Last mean reward per episode: 7.2e+01\n",
      "Num timesteps: 8940\n",
      "Best mean reward: 7.2e+01 - Last mean reward per episode: 7.2e+01\n",
      "Num timesteps: 8960\n",
      "Best mean reward: 7.2e+01 - Last mean reward per episode: 7.2e+01\n",
      "Num timesteps: 8980\n",
      "Best mean reward: 7.2e+01 - Last mean reward per episode: 7.3e+01\n",
      "Saving new best model at 8980 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9020\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9040\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9060\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Saving new best model at 9042 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9080\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9100\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9120\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9140\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9160\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9180\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9200\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.3e+01\n",
      "Num timesteps: 9220\n",
      "Best mean reward: 7.3e+01 - Last mean reward per episode: 7.5e+01\n",
      "Saving new best model at 9218 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9240\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9260\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9280\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9300\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9320\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Saving new best model at 9313 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9340\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9360\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9380\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9400\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9420\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9440\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9460\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.5e+01\n",
      "Num timesteps: 9480\n",
      "Best mean reward: 7.5e+01 - Last mean reward per episode: 7.6e+01\n",
      "Saving new best model at 9472 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9500\n",
      "Best mean reward: 7.6e+01 - Last mean reward per episode: 7.6e+01\n",
      "Num timesteps: 9520\n",
      "Best mean reward: 7.6e+01 - Last mean reward per episode: 7.6e+01\n",
      "Num timesteps: 9540\n",
      "Best mean reward: 7.6e+01 - Last mean reward per episode: 7.6e+01\n",
      "Num timesteps: 9560\n",
      "Best mean reward: 7.6e+01 - Last mean reward per episode: 7.6e+01\n",
      "Num timesteps: 9580\n",
      "Best mean reward: 7.6e+01 - Last mean reward per episode: 7.6e+01\n",
      "Num timesteps: 9600\n",
      "Best mean reward: 7.6e+01 - Last mean reward per episode: 7.6e+01\n",
      "Num timesteps: 9620\n",
      "Best mean reward: 7.6e+01 - Last mean reward per episode: 7.7e+01\n",
      "Saving new best model at 9603 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9640\n",
      "Best mean reward: 7.7e+01 - Last mean reward per episode: 7.7e+01\n",
      "Num timesteps: 9660\n",
      "Best mean reward: 7.7e+01 - Last mean reward per episode: 7.7e+01\n",
      "Num timesteps: 9680\n",
      "Best mean reward: 7.7e+01 - Last mean reward per episode: 7.7e+01\n",
      "Num timesteps: 9700\n",
      "Best mean reward: 7.7e+01 - Last mean reward per episode: 7.7e+01\n",
      "Num timesteps: 9720\n",
      "Best mean reward: 7.7e+01 - Last mean reward per episode: 7.8e+01\n",
      "Saving new best model at 9715 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9740\n",
      "Best mean reward: 7.8e+01 - Last mean reward per episode: 7.8e+01\n",
      "Num timesteps: 9760\n",
      "Best mean reward: 7.8e+01 - Last mean reward per episode: 7.8e+01\n",
      "Num timesteps: 9780\n",
      "Best mean reward: 7.8e+01 - Last mean reward per episode: 7.8e+01\n",
      "Num timesteps: 9800\n",
      "Best mean reward: 7.8e+01 - Last mean reward per episode: 7.8e+01\n",
      "Num timesteps: 9820\n",
      "Best mean reward: 7.8e+01 - Last mean reward per episode: 7.9e+01\n",
      "Saving new best model at 9820 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9840\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n",
      "Num timesteps: 9860\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n",
      "Num timesteps: 9880\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n",
      "Num timesteps: 9900\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n",
      "Num timesteps: 9920\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n",
      "Num timesteps: 9940\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n",
      "Saving new best model at 9922 timesteps to tmp/gym/best_model.zip\n",
      "Num timesteps: 9960\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n",
      "Num timesteps: 9980\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 7.9e+01 - Last mean reward per episode: 7.9e+01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x13de848e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create log dir.\n",
    "log_dir = \"tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment.\n",
    "env = make_vec_env('CartPole-v1', n_envs=1, monitor_dir=log_dir)\n",
    "# This is equivalent to:\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env = Monitor(env, log_dir)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Create Callback:\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=20, log_dir=log_dir, verbose=1)\n",
    "\n",
    "# Create and train the model.\n",
    "model = A2C('MlpPolicy', env, verbose=0)\n",
    "model.learn(total_timesteps=10000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Realtime plotting of performance\n",
    "[AH: Doesn't seem to work with VS Code notebooks.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "\n",
    "class PlottingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for plotting the performance in realtime.\n",
    "\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=1):\n",
    "        super(PlottingCallback, self).__init__(verbose)\n",
    "        self._plot = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # get the monitor's data\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if self._plot is None: # make the plot\n",
    "            plt.ion()\n",
    "            fig = plt.figure(figsize=(6,3))\n",
    "            ax = fig.add_subplot(111)\n",
    "            line, = ax.plot(x, y)\n",
    "            self._plot = (line, ax, fig)\n",
    "            plt.show()\n",
    "        else: # update and rescale the plot\n",
    "            self._plot[0].set_data(x, y)\n",
    "            self._plot[-2].relim()\n",
    "            self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02, \n",
    "                                    self.locals[\"total_timesteps\"] * 1.02])\n",
    "            self._plot[-2].autoscale_view(True,True,True)\n",
    "            self._plot[-1].canvas.draw()\n",
    "        \n",
    "# Create log dir\n",
    "log_dir = \"tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env('MountainCarContinuous-v0', n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "plotting_callback = PlottingCallback()\n",
    "        \n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "model.learn(20000, callback=plotting_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Progress bar\n",
    "[AH: Low value.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:25<00:00, 79.47it/s] \n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class ProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param pbar: (tqdm.pbar) Progress bar object\n",
    "    \"\"\"\n",
    "    def __init__(self, pbar):\n",
    "        super(ProgressBarCallback, self).__init__()\n",
    "        self._pbar = pbar\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Update the progress bar:\n",
    "        self._pbar.n = self.num_timesteps\n",
    "        self._pbar.update(0)\n",
    "\n",
    "# this callback uses the 'with' block, allowing for correct initialisation and destruction\n",
    "class ProgressBarManager(object):\n",
    "    def __init__(self, total_timesteps): # init object with total timesteps\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "        \n",
    "    def __enter__(self): # create the progress bar and callback, return the callback\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "            \n",
    "        return ProgressBarCallback(self.pbar)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb): # close the callback\n",
    "        self.pbar.n = self.total_timesteps\n",
    "        self.pbar.update(0)\n",
    "        self.pbar.close()\n",
    "        \n",
    "model = TD3('MlpPolicy', 'Pendulum-v0', verbose=0)\n",
    "with ProgressBarManager(2000) as callback: # this the garanties that the tqdm progress bar closes correctly\n",
    "    model.learn(2000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Composition\n",
    "* Several callbacks can do a composition into a single callback (e.g. save best model, show progress bar...).\n",
    "* To do that, a list is passed to `learn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1210/10000 [00:00<00:05, 1699.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2.2e+01\n",
      "Saving new best model at 980 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1900/10000 [00:01<00:04, 1700.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2000\n",
      "Best mean reward: 2.2e+01 - Last mean reward per episode: 2.2e+01\n",
      "Saving new best model at 1978 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3233/10000 [00:03<00:05, 1228.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3000\n",
      "Best mean reward: 2.2e+01 - Last mean reward per episode: 2.6e+01\n",
      "Saving new best model at 2971 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 3989/10000 [00:03<00:04, 1344.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4000\n",
      "Best mean reward: 2.6e+01 - Last mean reward per episode: 2.8e+01\n",
      "Saving new best model at 3991 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5177/10000 [00:05<00:04, 1204.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5000\n",
      "Best mean reward: 2.8e+01 - Last mean reward per episode: 3.3e+01\n",
      "Saving new best model at 4972 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6027/10000 [00:06<00:02, 1539.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6000\n",
      "Best mean reward: 3.3e+01 - Last mean reward per episode: 3.6e+01\n",
      "Saving new best model at 5948 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7231/10000 [00:07<00:02, 1200.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7000\n",
      "Best mean reward: 3.6e+01 - Last mean reward per episode: 4e+01\n",
      "Saving new best model at 6857 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8079/10000 [00:08<00:01, 1217.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 8000\n",
      "Best mean reward: 4e+01 - Last mean reward per episode: 4.8e+01\n",
      "Saving new best model at 7975 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 9202/10000 [00:10<00:00, 1153.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 9000\n",
      "Best mean reward: 4.8e+01 - Last mean reward per episode: 5.4e+01\n",
      "Saving new best model at 8916 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10217it [00:10, 1593.83it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 10000\n",
      "Best mean reward: 5.4e+01 - Last mean reward per episode: 6.3e+01\n",
      "Saving new best model at 9955 timesteps to tmp/gym/best_model.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 834.08it/s]\n"
     ]
    }
   ],
   "source": [
    "#from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_vec_env('CartPole-v1', n_envs=1, monitor_dir=log_dir)\n",
    "\n",
    "# Create callbacks\n",
    "auto_save_callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "with ProgressBarManager(10000) as progress_callback:\n",
    "  # This is equivalent to callback=CallbackList([progress_callback, auto_save_callback])\n",
    "  model.learn(10000, callback=[progress_callback, auto_save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
