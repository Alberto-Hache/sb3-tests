{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines3 Tutorial - Callbacks and hyperparameter tuning\n",
    "(Taken from <https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#>)\n",
    "- Comparing default and beest hyperparameters in RL.\n",
    "- Using callbacks for monitoring, auto-saving, model manipulation, progress bars...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: swig, tqdm\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter tuning\n",
    "We'll compare here the performance of \"Soft Actor Critic\" on the Pendulum environment with default and \"tuned\" hyperparameters.\n",
    "\n",
    "Resources:\n",
    "- rl zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "- Optuna: https://github.com/optuna/optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = Monitor(gym.make('Pendulum-v0')) # AH: Wrapped with Monitor to prevent erroneous metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v0'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.36e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 136       |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 20.3      |\n",
      "|    critic_loss     | 0.968     |\n",
      "|    ent_coef        | 0.812     |\n",
      "|    ent_coef_loss   | -0.337    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.55e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 129       |\n",
      "|    time_elapsed    | 12        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 43.2      |\n",
      "|    critic_loss     | 0.943     |\n",
      "|    ent_coef        | 0.644     |\n",
      "|    ent_coef_loss   | -0.662    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.52e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 127       |\n",
      "|    time_elapsed    | 18        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 73        |\n",
      "|    critic_loss     | 0.875     |\n",
      "|    ent_coef        | 0.516     |\n",
      "|    ent_coef_loss   | -0.886    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.39e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 124       |\n",
      "|    time_elapsed    | 25        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 84.9      |\n",
      "|    critic_loss     | 7.05      |\n",
      "|    ent_coef        | 0.422     |\n",
      "|    ent_coef_loss   | -0.875    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.36e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 125       |\n",
      "|    time_elapsed    | 31        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 98.2      |\n",
      "|    critic_loss     | 11.2      |\n",
      "|    ent_coef        | 0.359     |\n",
      "|    ent_coef_loss   | -0.813    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.3e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 116      |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.315    |\n",
      "|    ent_coef_loss   | -0.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.27e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 124       |\n",
      "|    time_elapsed    | 44        |\n",
      "|    total_timesteps | 5600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 135       |\n",
      "|    critic_loss     | 8.01      |\n",
      "|    ent_coef        | 0.269     |\n",
      "|    ent_coef_loss   | -0.561    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 5499      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 124       |\n",
      "|    time_elapsed    | 51        |\n",
      "|    total_timesteps | 6400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 136       |\n",
      "|    critic_loss     | 6.61      |\n",
      "|    ent_coef        | 0.228     |\n",
      "|    ent_coef_loss   | -0.352    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 6299      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.12e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 36        |\n",
      "|    fps             | 123       |\n",
      "|    time_elapsed    | 58        |\n",
      "|    total_timesteps | 7200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 158       |\n",
      "|    critic_loss     | 6.29      |\n",
      "|    ent_coef        | 0.2       |\n",
      "|    ent_coef_loss   | -0.19     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7099      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.11e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 40        |\n",
      "|    fps             | 123       |\n",
      "|    time_elapsed    | 64        |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 172       |\n",
      "|    critic_loss     | 6.62      |\n",
      "|    ent_coef        | 0.184     |\n",
      "|    ent_coef_loss   | 0.133     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 7899      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "default_model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1, seed=0, batch_size=64, policy_kwargs=dict(net_arch=[64, 64])).learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-182.34 +/- 97.99\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(default_model, eval_env, n_eval_episodes=500)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ph/1pjt_w7d2_38ytbzfhc_mwgm0000gn/T/ipykernel_51467/676753378.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Pendulum-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_arch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SAC' is not defined"
     ]
    }
   ],
   "source": [
    "tuned_model = SAC('MlpPolicy', 'Pendulum-v0', batch_size=256, verbose=1, policy_kwargs=dict(net_arch=[256, 256]), seed=0).learn(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-141.48 +/- 90.55\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(tuned_model, eval_env, n_eval_episodes=500)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Callbacks\n",
    "Callback = function that will be called at a given stage of the training.\n",
    "They are passed as an argument of `model.learn()`.\n",
    "\n",
    "Types:\n",
    "* Custom callback: called at 5 specific moments of the training process.\n",
    "* Event callback: called when a certain user-defined situation is detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Custom callback\n",
    "The class derives from `BaseCallback`.\n",
    "\n",
    "Events:\n",
    "* `_on_training_start`  Called before the first rollout starts.\n",
    "* `_on_rollout_start`   \n",
    "    * Rollout = collection of environment interactions using current policy.\n",
    "    * Triggered before collecting new samples.\n",
    "    * For off-policy algorithms, rollout = steps taken in the eenv between two updates.\n",
    "* `_on_step`\n",
    "    * Called by the model after each call to `env.step()`.\n",
    "    * For child callback (of an `EventCallback`), this is called when the event is triggered.\n",
    "    * :return: (bool) If False, training is aborted early.\n",
    "* `_on_rollout_end`     Triggered before updating the policy.\n",
    "* `_on_training_end`    Triggered before exiting the `learn()` method.\n",
    "\n",
    "Variables accessible in the callback:\n",
    "* `self.model`          The RL model (`type: BaseAlgorithm`).\n",
    "* `self.training_env`   The environment used for training (`type Union[gym.Env, VecEnv, None]`).\n",
    "* `self.n_calls`        Number of times the callback was called (`type int`).\n",
    "* `self.num_timesteps`  Total number of steps taken (number of envs x step calls) (`type int`).\n",
    "* `self.locals`         Local variables (`type: Dict[str, Any]`).\n",
    "* `self.globals`        Global variables (`type: Dict[str, Any]`).\n",
    "* `self.logger`         The logger object, to report on terminal (`type: stable_baselines3.common.logger`).\n",
    "* `self.parent`         The parent object (`type: Optional[BaseCallback]`).\n",
    "\n",
    "### 2.2 Event callback\n",
    "The class `EventCallback` derives from `BaseCallback`.\n",
    "When an event is triggered (e.g. `EvalCallback` when there's a new best model) =>\n",
    "a child callback is called (e.g. `StopTrainingOnRewardThreshold` if mean reward > a threshold).\n",
    "\n",
    "Callback collection:\n",
    "* Save the model periodically (`CheckpointCallback`)\n",
    "* Evaluate the model periodically and save the best one (`EvalCallback`)\n",
    "* Chain callbacks (`CallbackList`)\n",
    "* Trigger callback on events (`Event Callback`, `EveryNTimesteps`)\n",
    "* Stop training early based on a reward threshold (`StopTrainingOnRewardThreshold`)\n",
    "\n",
    "(Note: when using multiple envs. the frequence must be calculated like this: `save_freq = max(save_freq // n_envs, 1)`\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CheckpointCallback\n",
    "Save the model periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x109ab33a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=1000, save_path='./logs/', name_prefix='rl_model')\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0')\n",
    "model.learn(2000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EvalCallback\n",
    "Evaluate the model periodically and save the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('Pendulum-v0')\n",
    "# Use deterministic actions for evaluation\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, best_model_save_path='./logs/',\n",
    "    log_path='./logs/', eval_freq=500,\n",
    "    deterministic=True, render=False)\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0')\n",
    "model.learn(5000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CallbackList\n",
    "A list of chained callbacks will be called sequentially. Alternatively pass a list of callbacks to `learn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/')\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('Pendulum-v0')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/best_model',\n",
    "                             log_path='./logs/results', eval_freq=500)\n",
    "# Create the callback list\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0')\n",
    "# Equivalent to:\n",
    "# model.learn(5000, callback=[checkpoint_callback, eval_callback])\n",
    "model.learn(5000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StopTrainingOnRewardThreshold\n",
    "Stop training early based on a reward threshold.\n",
    "\n",
    "It must be used with the EvalCallback and use the event triggered by a new best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make('Pendulum-v0')\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-200, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n",
    "\n",
    "model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "# Almost infinite number of timesteps, but the training will stop\n",
    "# early as soon as the reward threshold is reached\n",
    "model.learn(int(1e10), callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EveryNTimesteps\n",
    "Trigger its child callback every n_steps timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EveryNTimesteps\n",
    "\n",
    "# this is equivalent to defining CheckpointCallback(save_freq=500)\n",
    "# checkpoint_callback will be triggered every 500 steps\n",
    "checkpoint_on_event = CheckpointCallback(save_freq=1, save_path='./logs/')\n",
    "event_callback = EveryNTimesteps(n_steps=500, callback=checkpoint_on_event)\n",
    "\n",
    "model = PPO('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "\n",
    "model.learn(int(2e4), callback=event_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StopTrainingOnMaxEpisodes\n",
    "Stop training at a maximum number of episodes (ignoring modelâ€™s total_timesteps).\n",
    "\n",
    "Note: For multiple environments it assumes max_episodes * n_envs episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "# Stops training when the model reaches the maximum number of episodes\n",
    "callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=5, verbose=1)\n",
    "\n",
    "model = A2C('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
    "# Almost infinite number of timesteps, but the training will stop\n",
    "# early as soon as the max number of episodes is reached\n",
    "model.learn(int(1e10), callback=callback_max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
