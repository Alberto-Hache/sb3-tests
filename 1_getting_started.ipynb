{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines3 Tutorial - Getting Started\n",
    "- Create a basic RL model.\n",
    "- Train it in a custom env.\n",
    "- Evaluate it with SB3 helper.\n",
    "\n",
    "Also:\n",
    "- Record a video of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of model and env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies: previously, install ffmpeg freeglut3-dev xvfb  (for visualization).\n",
    "\n",
    "# Imports\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Import the RL model.\n",
    "from stable_baselines3 import PPO # RL algorithm.\n",
    "# from stable_baselines3.ppo.policies import MlpPolicy # Not needed if specified at model creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gym env:\n",
    "# The action space is deduced from the env. action space.\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Instantiate the agent:\n",
    "# - PPO (actor-critic, using a value function to improve the policy).\n",
    "# - MlpPolicy because the observation of the CartPole task is a feature vector, not images.\n",
    "# model = PPO(MlpPolicy, env, verbose=0) # Not recommended, see Note below.\n",
    "model = PPO('MlpPolicy', env, verbose=0)\n",
    "\n",
    "# NOTE: Some algorithms like SAC have their own MlpPolicy => using string for the policy is the recommened option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the agent.\n",
    "def evaluate(model, num_episodes=100):\n",
    "    env = model.get_env() # Use the env. of the model.\n",
    "    all_episode_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset() # Initialize env; first observation.\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs) # states are only useful with LSTM policies.\n",
    "            obs, reward, done, info = env.step(action) # returns arrays (bc vectorized env.)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "    \n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(f'Mean reward: {mean_episode_reward}, Num episodes: {num_episodes}')\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 22.700000762939453, Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "# Random agent, before training\n",
    "mean_reward_before_train = evaluate(model, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AlbertoH/projects/sb3-tests/.venv/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 114.24 +/- 15.04\n"
     ]
    }
   ],
   "source": [
    "# Using Stable Baselines 3's helper.\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "print(f'Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 432.05 +/- 95.85\n"
     ]
    }
   ],
   "source": [
    "# Train the agent and evaluate it.\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "print(f'Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail.\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_XSERVTransmkdir: ERROR: euid != 0,directory /tmp/.X11-unix will not be created.\n",
      "_XSERVTransSocketUNIXCreateListener: mkdir(/tmp/.X11-unix) failed, errno = 2\n",
      "_XSERVTransMakeAllCOTSServerListeners: failed to create listener for local\n",
      "(EE) \n",
      "Fatal server error:\n",
      "(EE) Cannot establish any listening sockets - Make sure an X server isn't already running(EE) \n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_videos(video_path='', prefix=''):\n",
    "  \"\"\"\n",
    "  Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "  :param video_path: (str) Path to the folder containing videos\n",
    "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "  \"\"\"\n",
    "  html = []\n",
    "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "      html.append('''<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
    "  \"\"\"\n",
    "  :param env_id: (str)\n",
    "  :param model: (RL model)\n",
    "  :param video_length: (int)\n",
    "  :param prefix: (str)\n",
    "  :param video_folder: (str)\n",
    "  \"\"\"\n",
    "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "  # Start the video at step=0 and record 500 steps\n",
    "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
    "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
    "                              name_prefix=prefix)\n",
    "\n",
    "  obs = eval_env.reset()\n",
    "  for _ in range(video_length):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "  # Close the video recorder\n",
    "  eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to /Users/AlbertoH/projects/sb3-tests/videos/ppo2-cartpole-step-0-to-step-500.mp4\n"
     ]
    }
   ],
   "source": [
    "# Record and save the video.\n",
    "record_video('CartPole-v1', model, video_length=500, prefix='ppo2-cartpole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative 1-line training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy class is inferred; the environment is automatically created.\n",
    "# This works because both are registered.\n",
    "\n",
    "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa8ddef7fe017c205a12249db0f3a795fe8e56e1fe12e11b503d179c21782f37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
