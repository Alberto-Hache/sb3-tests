{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies in SB3\n",
    "Notes taken from SB3 documention:\n",
    "<https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html#>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements for Atari\n",
    "\n",
    "'cmake' is required for 'autorom', installed within 'gym' with atari.\n",
    "\n",
    "!pip install cmake\n",
    "!pip install 'gym[atari,accept-rom-license]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom policy network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | 22       |\n",
      "| time/              |          |\n",
      "|    fps             | 1750     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.2        |\n",
      "|    ep_rew_mean          | 26.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1226        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009562779 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | 0.00774     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 74.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.1        |\n",
      "|    ep_rew_mean          | 37.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1144        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006418679 |\n",
      "|    clip_fraction        | 0.0162      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.674      |\n",
      "|    explained_variance   | -0.182      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00968    |\n",
      "|    value_loss           | 68.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.5        |\n",
      "|    ep_rew_mean          | 47.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1113        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006077492 |\n",
      "|    clip_fraction        | 0.0228      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | -0.23       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 59          |\n",
      "|    ep_rew_mean          | 59          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1096        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004952864 |\n",
      "|    clip_fraction        | 0.0161      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.634      |\n",
      "|    explained_variance   | -0.0635     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00901    |\n",
      "|    value_loss           | 75.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x13974a460>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom actor (pi) and value function (vf) networks\n",
    "# of two layers of size 32 each with Relu activation function\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=[dict(pi=[32, 32], vf=[32, 32])])\n",
    "# Create the agent\n",
    "model = PPO(\"MlpPolicy\", \"CartPole-v1\", policy_kwargs=policy_kwargs, verbose=1)\n",
    "# Retrieve the environment\n",
    "env = model.get_env()\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(\"tmp/ppo_cartpole\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back: the policy_kwargs are automatically loaded\n",
    "model = PPO.load(\"tmp/ppo_cartpole\", env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Feature Extractor\n",
    "* Feature extractor is shared by default between actor and critic (when applicable).\n",
    "* This can be changed with share_features_extractor=False in policy_kwargs for off-policy algorthms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 652      |\n",
      "|    ep_rew_mean     | 0.846    |\n",
      "| time/              |          |\n",
      "|    fps             | 320      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x13a4cf5e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of custom CNN for images as input.\n",
    "import gym\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "# Environment definition.\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "env = make_atari_env('BreakoutNoFrameskip-v4', n_envs=1, seed=0)\n",
    "\n",
    "# Define model and train it.\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Inputs and Dictionary Observations\n",
    "(Taken from <https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html#multiple-inputs-and-dictionary-observations>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy algorithms\n",
    "(Taken from <https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html#on-policy-algorithms>)\n",
    "- Use of 'net_arch' parameter for A2C and PPO policies.\n",
    "- 'net_arch' defines the architecture of the shared network (if any) and the policy and value functions.\n",
    "\n",
    "E.g.:\n",
    "* Two shared layers of size 128: `net_arch=[128, 128]`\n",
    "\n",
    "* Value network deeper than policy network, first layer shared: `net_arch=[128, dict(vf=[256, 256])]`\n",
    "\n",
    "* Initially shared then diverging: `[128, dict(vf=[256], pi=[16])]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy algorithms\n",
    "(Taken from <https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html#off-policy-algorithms>)\n",
    "* Use of 'net_arch' parameter for SAC, DDPG or TD3.\n",
    "* 'net_arch' can define both networks.\n",
    "\n",
    "E.g.:\n",
    "* Different architectures:`net_arch=dict(qf=[400, 300], pi=[64, 64])`\n",
    "* Shared architecture: `net_arch=[256, 256]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
